{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dad606e-9bec-4bec-af06-948628e9060e",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "```\n",
    "CSV files required as preocnditions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e9e53-ec96-4435-ad77-5cf3a6ba964a",
   "metadata": {},
   "source": [
    "### Make anomaly datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0de077e-5b44-479e-8f5e-be4cee1879cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def write_anomaly_datasets_with_csv(input_path, output_path, anomaly_rate=1):\n",
    "    df = pd.read_csv(input_path)\n",
    "    target_name = df.columns[-1]\n",
    "    targets = np.array(df[target_name])\n",
    "    threshold = np.percentile(targets, 100-anomaly_rate)\n",
    "    anomaly_target = []\n",
    "    for i in targets:\n",
    "        anomaly_target.append(0 if i <= threshold else 1)\n",
    "    modified_df = df.copy()\n",
    "    modified_df = modified_df.drop(columns=target_name)\n",
    "    modified_df['Anomaly'] = np.array(anomaly_target)\n",
    "\n",
    "    modified_df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "def make_input_output_path(target, poc, delay):\n",
    "    input_path = f'./datasets/{poc}/{poc}_{target[:3].lower()}_{delay}hr.csv'\n",
    "    if not os.path.exists('./anomaly_datasets'):\n",
    "        os.makedirs('./anomaly_datasets')\n",
    "    if not os.path.exists(f'./anomaly_datasets/{poc}'):\n",
    "        os.makedirs(f'./anomaly_datasets/{poc}')\n",
    "    output_path = f'./anomaly_datasets/{poc}/{poc}_{target[:3].lower()}_{delay}hr.csv'\n",
    "\n",
    "    return input_path, output_path\n",
    "\n",
    "\n",
    "targets = ['Avg$PerMWHr', 'Med$PerMWHr']\n",
    "pocs = [\n",
    "        'ALB0331',   ## Auckland\n",
    "        'HAM0331',   ## Hamilton\n",
    "        'WIL0331',   ## Wellington\n",
    "        'ISL0661',   ## ChristChurch\n",
    "        'SDN0331',   ## Dunedin\n",
    "        'STK0331',   ## Nelson\n",
    "       ]\n",
    "delays = [\n",
    "    0.5, 4, 6, 24\n",
    "]\n",
    "# Percentage of defined anomalies in the datasets\n",
    "anomaly_rate = [0.1, 0.5, 1, 2]\n",
    "\n",
    "for r in anomaly_rate:\n",
    "    for t in targets:\n",
    "        for p in pocs:\n",
    "            for d in delays:\n",
    "                input, output = make_input_output_path(t, p, d)\n",
    "                write_anomaly_datasets_with_csv(input, output[:-4]+'_'+str(r)+output[-4:], r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe5be4-20be-4966-84b0-2263931ebba6",
   "metadata": {},
   "source": [
    "### Define experimental loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae301835-e59d-4486-88d0-b78244a44692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "from river import anomaly\n",
    "from river import stream\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import random\n",
    "\n",
    "datasets_folder = './'\n",
    "def _evaluate_hst_river(file_name, window_size=256, height=8, n_trees=100, seed=None, shuffle=True, skip_anomaly=False):\n",
    "    df = pd.read_csv(file_name)\n",
    "    X = df.iloc[:, :-1]  # features\n",
    "    y = df.iloc[:, -1].astype(int)  # label\n",
    "\n",
    "    X = (X - X.min()) / (X.max() - X.min())  # normalize\n",
    "\n",
    "    model = anomaly.HalfSpaceTrees(n_trees=n_trees, height=height, window_size=window_size, seed=seed)\n",
    "\n",
    "    dataset = stream.iter_pandas(X, y)\n",
    "    scores = []\n",
    "\n",
    "    start = time.time()\n",
    "    for i, (x, l) in enumerate(dataset):\n",
    "        s = model.score_one(x)\n",
    "        scores.append(float(s))\n",
    "        if l == 1 and skip_anomaly:\n",
    "            continue\n",
    "        model.learn_one(x)\n",
    "    end = time.time()\n",
    "\n",
    "    auc = roc_auc_score(y, scores)\n",
    "    pr = average_precision_score(y, scores)\n",
    "    return auc, pr, end - start\n",
    "\n",
    "\n",
    "def evaluate_hst_river(file_name, window_size=256, height=8, n_trees=100, runs=10, shuffle=True, skip_anomaly=False, print_each_result=False):\n",
    "    auc_results = []\n",
    "    pr_results = []\n",
    "    runtimes = []\n",
    "    records = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(_evaluate_hst_river, file_name, window_size, height, n_trees, run, shuffle,\n",
    "                                   skip_anomaly): run for run in range(runs)}\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        auc, pr, elapse = future.result()\n",
    "        auc_results.append(auc)\n",
    "        pr_results.append(pr)\n",
    "        runtimes.append(elapse)\n",
    "        records.append((file_name, 'HST-RIVER', auc, pr, elapse, futures[future], shuffle))\n",
    "    if print_each_result:\n",
    "        print(f\"AUC mean: {np.mean(auc_results)} , AUC std: {np.std(auc_results)}\")\n",
    "        print(f\"PR mean: {np.mean(pr_results)}, PR std: {np.std(pr_results)}\")\n",
    "        print(f\"Runtime mean: {np.mean(runtimes)}, Runtime std: {np.std(runtimes)}\")\n",
    "    df = pd.DataFrame(records, columns=['dataset', 'method', 'auc', 'pr', 'runtime', 'seed', 'shuffled'])\n",
    "    # df.to_csv(os.path.join(record_folder, 'aif-ecai-records.csv'), index=False, header=None,  mode='a')\n",
    "    return np.mean(auc_results), np.mean(pr_results), np.mean(runtimes), df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2460814-061c-42eb-a17b-0035cb420536",
   "metadata": {},
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd455fcc-d7b8-4d20-9fc7-7024d3273600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dateset_name(poc, delay, type, ratio):\n",
    "    return f'./anomaly_datasets/{poc}/{poc}_{type}_{delay}hr_{ratio}.csv'\n",
    "\n",
    "\n",
    "pocs = [\n",
    "        'ALB0331',   ## Auckland\n",
    "        'HAM0331',   ## Hamilton\n",
    "        'WIL0331',   ## Wellington\n",
    "        'ISL0661',   ## ChristChurch\n",
    "        'SDN0331',   ## Dunedin\n",
    "        'STK0331',   ## Nelson\n",
    "       ]\n",
    "\n",
    "results_dfs = []\n",
    "p_col = []\n",
    "t_col = []\n",
    "d_col = []\n",
    "r_col = []\n",
    "auc_col = []\n",
    "\n",
    "for p in pocs:\n",
    "    for d in ['0.5','4','6','24']:\n",
    "        for t in ['avg', 'med']:\n",
    "            for r in ['0.1', '0.5', '1', '2']:\n",
    "                file_name = make_dateset_name(p,d,t,r)\n",
    "                auc, pr, time111, single_run_df = evaluate_hst_river(file_name=file_name, runs=3)\n",
    "                p_col.append(p)\n",
    "                t_col.append(t)\n",
    "                d_col.append(d)\n",
    "                r_col.append(r)\n",
    "                auc_col.append(auc)\n",
    "                results_dfs.append(single_run_df)\n",
    "\n",
    "data ={\n",
    "    \"PoC\":p_col,\n",
    "    \"Target\":t_col,\n",
    "    \"Delay\":d_col,\n",
    "    \"Ratio\":r_col,\n",
    "    \"AUC ROC\":auc_col,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99762ec4-53c8-4b7b-aedd-0e959a30b748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
